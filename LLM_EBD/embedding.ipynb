{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import os\n",
    "import pickle\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline, logging, Trainer, AutoConfig, LlamaConfig\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "\n",
    "subprocess.run([\"huggingface-cli\", \"login\", \"--token\", \"[your token]\"])\n",
    "\n",
    "def open_data_dict(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        contents_dict = pickle.load(file)\n",
    "        contents_dict = {int(key.split('.')[0]): value for key, value in contents_dict.items()}\n",
    "    return contents_dict\n",
    "\n",
    "input_dict = open_data_dict('/data/log-data-2024/20241123_Final/content_meta_dict_20241123.pickle')\n",
    "input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 토크나이저 설정\n",
    "model_ckpt = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "\n",
    "# 토큰 길이 계산\n",
    "token_lengths = []\n",
    "\n",
    "for key, text in input_dict.items():\n",
    "    tokens = tokenizer(text, truncation=False)['input_ids']  # 토큰화\n",
    "    token_lengths.append(len(tokens))  # 토큰 길이 추가\n",
    "\n",
    "# 토큰 길이 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(token_lengths, bins=30, color='blue', alpha=0.7)\n",
    "plt.title(\"Distribution of Token Lengths\")\n",
    "plt.xlabel(\"Token Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 및 토크나이저 설정\n",
    "model_ckpt = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=False\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_ckpt,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=bnb_config,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map='auto'\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_config.inference_mode = False\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Dictionary를 텍스트로 변환\n",
    "embeddings_dict = {}\n",
    "\n",
    "# 각 키에 대해 임베딩 계산\n",
    "for key, text in tqdm(input_dict.items(), desc=\"Processing Texts\", unit=\"entry\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=False)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_hidden_states=True)\n",
    "        embeddings = outputs.hidden_states[-1]  # 마지막 히든 레이어를 임베딩으로 사용 (torch.Size([1, N, 4096]), N은 토큰의 수)\n",
    "        mean_embeddings = embeddings.mean(dim=1)  # [1, N, 4096] -> [1, 4096]로 평균화\n",
    "        embeddings_dict[key] = mean_embeddings.cpu().numpy()  # 키와 임베딩을 dictionary에 저장\n",
    "        embeddings_dict[key] = mean_embeddings.to(dtype=torch.float16)  # 키와 임베딩을 torch.float16 형태로 저장\n",
    "\n",
    "embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_zero_tensor(embeddings_dict):\n",
    "    zero_tensor = torch.zeros((1, 4096), dtype=torch.float16)\n",
    "    embeddings_dict[0] = zero_tensor\n",
    "    return embeddings_dict\n",
    "\n",
    "embeddings_dict = add_zero_tensor(embeddings_dict)\n",
    "print(embeddings_dict[0])\n",
    "print(embeddings_dict[0].size())\n",
    "\n",
    "print(len(embeddings_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 .pt 파일로 저장\n",
    "output_file_path = './data/embedding_vec_dict_241123.pt'\n",
    "torch.save(embeddings_dict, output_file_path)\n",
    "\n",
    "print(f\"Embeddings dictionary saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import torch\n",
    "\n",
    "embedding_matrix = torch.stack([embeddings_dict[key].squeeze(0) for key in embeddings_dict.keys()])\n",
    "embedding_matrix = embedding_matrix.view(embedding_matrix.size(0), -1)\n",
    "\n",
    "pca = PCA(n_components=128)  # or 512, 256, 128 ()\n",
    "reduced_embeddings = pca.fit_transform(embedding_matrix.cpu().numpy())\n",
    "reduced_embeddings = torch.tensor(reduced_embeddings, dtype=torch.float16)\n",
    "embedding_vec_dict_reduced = {key: reduced_embeddings[i] for i, key in enumerate(embeddings_dict.keys())}\n",
    "\n",
    "torch.save(embedding_vec_dict_reduced, './data/embedding_vec_dict_241123_reduced_128.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
